{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43f8354e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print('all ok')\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baa6654e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e3e72bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<think>\\nOkay, the user asked who I am. First, I need to introduce my identity, which is Qwen, a large-scale language model developed by Alibaba. Then, I should mention my main functions and application scenarios, such as answering questions, creating text, logical reasoning, coding, etc. The user may also be interested in my multilingual support and other features, so I should highlight that. I need to keep the tone friendly and natural, avoiding overly technical terms. Finally, I can ask if the user needs help with anything, maintaining a conversational style. I should also check if there's any additional information the user might need beyond the basics.\\n</think>\\n\\nHello! I'm Qwen, a large-scale language model developed by Alibaba. I can help with a variety of tasks, including answering questions, creating text (like stories, documents, emails, and scripts), logical reasoning, coding, and more. I support multiple languages, including Chinese, English, German, French, Spanish, Portuguese, Italian, Dutch, Russian, Czech, Polish, Arabic, Persian, Hebrew, Turkish, Japanese, Korean, and many others. Is there anything specific you'd like help with? ðŸ˜Š\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ChatGroq(model='qwen/qwen3-32b')\n",
    "model.invoke(\"who are you?\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16e67197",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\LLMOpsPractice\\Documentportal\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "hf_embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "#embeddings = hf_embeddings.embed_query(\"Hello world\")\n",
    "#print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe3263e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde283aa",
   "metadata": {},
   "source": [
    "# Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5b73a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "287c8e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "026077c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\LLMOpsPractice\\\\Documentportal\\\\notebook'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e9b1900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\LLMOpsPractice\\\\Documentportal\\\\notebook\\\\data\\\\sample.pdf'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = os.path.join(os.getcwd(),'data','sample.pdf')\n",
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c326c948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = PyPDFLoader(file_path)\n",
    "documents = loader.load()\n",
    "len(documents)  # 77pages have pdf, which is 77 documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "119a0892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the experiment value , there is no specfic value for chunk_size and chunk_overlap\n",
    "# it depends on the use case, for example if you want to chunk the text into smaller\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=150,length_function=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fc8d169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "765"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs =text_splitter.split_documents(documents)\n",
    "len(docs) # now we will be having 765 documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efe8adab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'producer': 'pdfTeX-1.40.25',\n",
       " 'creator': 'LaTeX with hyperref',\n",
       " 'creationdate': '2023-07-20T00:30:36+00:00',\n",
       " 'author': '',\n",
       " 'keywords': '',\n",
       " 'moddate': '2023-07-20T00:30:36+00:00',\n",
       " 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
       " 'subject': '',\n",
       " 'title': '',\n",
       " 'trapped': '/False',\n",
       " 'source': 'e:\\\\LLMOpsPractice\\\\Documentportal\\\\notebook\\\\data\\\\sample.pdf',\n",
       " 'total_pages': 77,\n",
       " 'page': 76,\n",
       " 'page_label': '77'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[764].metadata\n",
    "#docs[764].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a2f7626",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef8cd6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#in memory store - RAM store\n",
    "vectorstore = FAISS.from_documents(docs, hf_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3a44b8",
   "metadata": {},
   "source": [
    "# Retrieval Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "26d30f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# means from the vectordatabase,we are going to fetch or retrieve or rank the most appropriate documents/Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f32fa96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_documents = vectorstore.similarity_search(\"llama2 fintuning benchmark expirements?\", k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4467197e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='348ddbe2-3990-4134-9e1a-5d83476ad9ec', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'e:\\\\LLMOpsPractice\\\\Documentportal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 48, 'page_label': '49'}, page_content='Llama 2\\n7B 77.4 78.8 48.3 77.2 69.2 75.2 45.9 58.6 57.8 45.3\\n13B 81.7 80.5 50.3 80.7 72.8 77.3 49.4 57.0 67.3 54.8\\n34B 83.7 81.9 50.9 83.3 76.7 79.4 54.5 58.2 74.3 62.6\\n70B 85.0 82.8 50.7 85.3 80.2 80.2 57.4 60.2 78.5 68.9\\nTable 20: Performance on standard benchmarks.\\nHuman-Eval MBPP\\npass@1 pass@100 pass@1 pass@80\\nMPT 7B 18.3 - 22.6 -\\n30B 25.0 - 32.8 -\\nFalcon 7B 0.0 - 11.2 -\\n40B 0.6 - 29.8 -\\nLlama 1\\n7B 10.5 36.5 17.7 56.2\\n13B 15.8 52.5 22.0 64.0\\n33B 21.7 70.7 30.2 73.4\\n65B 23.7 79.3 37.7 76.8'),\n",
       " Document(id='1534b93e-4a78-48d6-933b-caa2893cdacd', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'e:\\\\LLMOpsPractice\\\\Documentportal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 76, 'page_label': '77'}, page_content='See evaluations for pretraining (Section 2); fine-tuning (Section 3); and safety (Section 4).\\nEthical Considerations and Limitations(Section 5.2)\\nLlama 2is a new technology that carries risks with use. Testing conducted to date has been in\\nEnglish, and has not covered, nor could it cover all scenarios. For these reasons, as with all LLMs,\\nLlama 2â€™s potential outputs cannot be predicted in advance, and the model may in some instances'),\n",
       " Document(id='9a7dcda1-80d4-4930-ada7-0ac08ad12cb4', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'e:\\\\LLMOpsPractice\\\\Documentportal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 76, 'page_label': '77'}, page_content='Llama 2.\\nHardware and Software(Section 2.2)\\nTraining Factors We used custom training libraries, Metaâ€™s Research Super Cluster, and produc-\\ntion clusters for pretraining. Fine-tuning, annotation, and evaluation were also\\nperformed on third-party cloud compute.\\nCarbon Footprint Pretraining utilized a cumulative 3.3M GPU hours of computation on hardware\\nof type A100-80GB (TDP of 350-400W). Estimated total emissions were 539\\ntCO2eq, 100% of which were offset by Metaâ€™s sustainability program.'),\n",
       " Document(id='462c8dfe-7df7-4b69-9655-4182a8d36d3f', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'e:\\\\LLMOpsPractice\\\\Documentportal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 73, 'page_label': '74'}, page_content='Llama 1\\n7B 0.27 0.26 0.34 0.54 0.36 0.39 0.26 0.28 0.33 0.45 0.33 0.17 0.24 0.31 0.44 0.57 0.39 0.3513B 0.24 0.24 0.31 0.52 0.37 0.37 0.23 0.28 0.31 0.50 0.27 0.10 0.24 0.27 0.41 0.55 0.34 0.2533B 0.23 0.26 0.34 0.50 0.36 0.35 0.24 0.33 0.34 0.49 0.31 0.12 0.23 0.30 0.41 0.60 0.28 0.2765B 0.25 0.26 0.34 0.46 0.36 0.40 0.25 0.32 0.32 0.48 0.31 0.11 0.25 0.30 0.43 0.60 0.39 0.34\\nLlama 2'),\n",
       " Document(id='7237c885-8d68-4201-b1d5-3ff00af2f914', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'e:\\\\LLMOpsPractice\\\\Documentportal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 76, 'page_label': '77'}, page_content='Llama 2â€™s potential outputs cannot be predicted in advance, and the model may in some instances\\nproduce inaccurate or objectionable responses to user prompts. Therefore, before deploying any\\napplications ofLlama 2, developers should perform safety testing and tuning tailored to their\\nspecific applications of the model. Please see the Responsible Use Guide available available at\\nhttps://ai.meta.com/llama/responsible-user-guide\\nTable 52: Model card forLlama 2.\\n77')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "84b21f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8d6a766c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='348ddbe2-3990-4134-9e1a-5d83476ad9ec', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'e:\\\\LLMOpsPractice\\\\Documentportal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 48, 'page_label': '49'}, page_content='Llama 2\\n7B 77.4 78.8 48.3 77.2 69.2 75.2 45.9 58.6 57.8 45.3\\n13B 81.7 80.5 50.3 80.7 72.8 77.3 49.4 57.0 67.3 54.8\\n34B 83.7 81.9 50.9 83.3 76.7 79.4 54.5 58.2 74.3 62.6\\n70B 85.0 82.8 50.7 85.3 80.2 80.2 57.4 60.2 78.5 68.9\\nTable 20: Performance on standard benchmarks.\\nHuman-Eval MBPP\\npass@1 pass@100 pass@1 pass@80\\nMPT 7B 18.3 - 22.6 -\\n30B 25.0 - 32.8 -\\nFalcon 7B 0.0 - 11.2 -\\n40B 0.6 - 29.8 -\\nLlama 1\\n7B 10.5 36.5 17.7 56.2\\n13B 15.8 52.5 22.0 64.0\\n33B 21.7 70.7 30.2 73.4\\n65B 23.7 79.3 37.7 76.8'),\n",
       " Document(id='1534b93e-4a78-48d6-933b-caa2893cdacd', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'e:\\\\LLMOpsPractice\\\\Documentportal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 76, 'page_label': '77'}, page_content='See evaluations for pretraining (Section 2); fine-tuning (Section 3); and safety (Section 4).\\nEthical Considerations and Limitations(Section 5.2)\\nLlama 2is a new technology that carries risks with use. Testing conducted to date has been in\\nEnglish, and has not covered, nor could it cover all scenarios. For these reasons, as with all LLMs,\\nLlama 2â€™s potential outputs cannot be predicted in advance, and the model may in some instances'),\n",
       " Document(id='9a7dcda1-80d4-4930-ada7-0ac08ad12cb4', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'e:\\\\LLMOpsPractice\\\\Documentportal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 76, 'page_label': '77'}, page_content='Llama 2.\\nHardware and Software(Section 2.2)\\nTraining Factors We used custom training libraries, Metaâ€™s Research Super Cluster, and produc-\\ntion clusters for pretraining. Fine-tuning, annotation, and evaluation were also\\nperformed on third-party cloud compute.\\nCarbon Footprint Pretraining utilized a cumulative 3.3M GPU hours of computation on hardware\\nof type A100-80GB (TDP of 350-400W). Estimated total emissions were 539\\ntCO2eq, 100% of which were offset by Metaâ€™s sustainability program.'),\n",
       " Document(id='462c8dfe-7df7-4b69-9655-4182a8d36d3f', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'e:\\\\LLMOpsPractice\\\\Documentportal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 73, 'page_label': '74'}, page_content='Llama 1\\n7B 0.27 0.26 0.34 0.54 0.36 0.39 0.26 0.28 0.33 0.45 0.33 0.17 0.24 0.31 0.44 0.57 0.39 0.3513B 0.24 0.24 0.31 0.52 0.37 0.37 0.23 0.28 0.31 0.50 0.27 0.10 0.24 0.27 0.41 0.55 0.34 0.2533B 0.23 0.26 0.34 0.50 0.36 0.35 0.24 0.33 0.34 0.49 0.31 0.12 0.23 0.30 0.41 0.60 0.28 0.2765B 0.25 0.26 0.34 0.46 0.36 0.40 0.25 0.32 0.32 0.48 0.31 0.11 0.25 0.30 0.43 0.60 0.39 0.34\\nLlama 2'),\n",
       " Document(id='7237c885-8d68-4201-b1d5-3ff00af2f914', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'e:\\\\LLMOpsPractice\\\\Documentportal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 76, 'page_label': '77'}, page_content='Llama 2â€™s potential outputs cannot be predicted in advance, and the model may in some instances\\nproduce inaccurate or objectionable responses to user prompts. Therefore, before deploying any\\napplications ofLlama 2, developers should perform safety testing and tuning tailored to their\\nspecific applications of the model. Please see the Responsible Use Guide available available at\\nhttps://ai.meta.com/llama/responsible-user-guide\\nTable 52: Model card forLlama 2.\\n77')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"llama2 fintuning benchmark expirements?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8a2343a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Question -> user question\n",
    "## COntext is -> retriving from the vector database ->based on the user question , \n",
    "# we are going to retriving the information from the vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2670141c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating chain with retriever\n",
    "prompt_template = \"\"\"You are a helpful assistant.Answer the question based on the context provided below\n",
    "if the context does not contain sufficient information, say \"I don't know\".\n",
    "context: {context}\n",
    "Question: {question}\n",
    "Answer:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eb5860a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "320bb953",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "30d0c0af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='You are a helpful assistant.Answer the question based on the context provided below\\nif the context does not contain sufficient information, say \"I don\\'t know\".\\ncontext: {context}\\nQuestion: {question}\\nAnswer:')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bec57c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Chain using LCEL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0a713b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8a6d4982",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain  = prompt | model |parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5327e210",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6c7c3990",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rag_chain.invoke(\"tell me about the llama2 fine tuning benchmark experiments?\")\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\":retriever | format_docs , \"question\":RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "711cd16f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nOkay, so the user is asking about the scaling trends for the reward model. Let me look through the context provided to find the relevant information.\\n\\nFirst, I notice there\\'s a mention of \"Figure 6: Scaling trends for the reward model. More data and a larger-size model generally improve.\" That seems directly related. Then there are two sets of data points with numbers like 0.52, 0.54 up to 0.64 and another set from 0.50 to 0.80. These might represent accuracy metrics at different stages. The models listed are 7b, 13b, 70b, GPT4, OpenAssistant. \\n\\nLooking at the text around Figure 6, it says that more data and larger models improve accuracy. The first figure shows accuracy on all examples, starting at 0.52 and increasing to 0.64, while the second shows accuracy on \"Significantly Better\" examples, starting at 0.50 and going up to 0.80. The models are ordered by size, from 7b to 70b, then GPT4 and OpenAssistant. \\n\\nThe context also mentions that gathering new preference data is important to keep the reward model accurate, and that iterative updates help prevent divergence. However, the actual scaling trends are depicted in the figures, so the answer should focus on that. Since the user is asking specifically about scaling trends, the key points are that as the model size increases (from 7b to 70b) and more data is used, the accuracy improves. The figures show a steady increase in accuracy metrics. The answer should mention that more data and larger models lead to better accuracy, as per the figures described. Also, the use of margin-based loss affects reward distribution, but that\\'s a separate point from scaling trends.\\n\\nI need to make sure not to include information about margin-based loss unless it\\'s relevant, which it\\'s not here. The main answer is based on Figure 6 and the text explaining that scaling up models and data improves accuracy. The user might not need the details about hyper-specialization or the last verification step unless they ask for more. So the answer is concise: scaling trends show that increasing model size and data amount improve reward model accuracy, as shown in the figures with specific accuracy ranges.\\n</think>\\n\\nThe scaling trends for the reward model indicate that **more data and larger model sizes generally improve accuracy**. Specifically:  \\n\\n- **Accuracy on all examples** increases from ~0.52 to ~0.64 (for model sizes 7b â†’ 13b â†’ 70b).  \\n- **Accuracy on \"Significantly Better\" examples** rises from ~0.50 to ~0.80 as model size and data scale.  \\n- Larger models (e.g., 70b) and iterative updates help maintain alignment with new distributions, preventing degradation from hyper-specialization.  \\n\\nThese trends are visualized in the provided figures (Figure 6) and suggest that both data quantity and model capacity are critical for reward model performance.'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"can you tell me Scaling trends for the reward model?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b37a5f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115ebde1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af591591",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6bace4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Documentportal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
